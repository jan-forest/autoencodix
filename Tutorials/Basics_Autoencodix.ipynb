{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a VAE with AUTOENCODIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using the example data from TCGA for Stomach Adenocarinoma.\n",
    "\n",
    "The Setup_InputFormat [notebook](Setup_InputFormat.ipynb) we ran earlier is the prerequisite for this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure you are using the correct kernel \n",
    "You probably have already selected **autoencodix-ga-kernel** (visible in the top right corner of this notebook) as instructed in `install-kernel-script.ipynb`, if not:\n",
    "\n",
    "Select the **autoencodix-ga-kernel** in the Top Menu -> Kernel -> Change Kernel\"\n",
    "\n",
    "If it is not listed, reload the browser and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Notebook Setup\n",
    "Firstly, we need to ensure we are working in the correct directory. The next cell ensures this, all other cells will assume you are in `/data/horse/ws/<YOURUSERNAME>-AE-ws/autoencodix/Tutorials` and will change directories based on this location if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /data/horse/ws/majo158f-AE-ws/autoencodix/Tutorials\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.path.expandvars(\"$HOME/horse/$USER-AE-ws/autoencodix/Tutorials\")\n",
    "os.chdir(path)\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "print(f\"Current working directory: {cur_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Step 1) Check your input data\n",
    "Input data must have shape samples x features and should be in `data/raw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet files:\n",
      "combined_rnaseq_formatted.parquet\n",
      "combined_clin_formatted.parquet\n",
      "combined_meth_formatted.parquet\n"
     ]
    }
   ],
   "source": [
    "raw_data_dir = Path(os.path.expandvars(\"$HOME/horse/$USER-AE-ws/autoencodix/data/raw/\"))\n",
    "parquet_files = list(raw_data_dir.glob(\"combined*.parquet\"))\n",
    "print(\"Parquet files:\")\n",
    "for file in parquet_files:\n",
    "    print(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Step 2) Create your yaml-config to training and pipeline specification\n",
    "A full documentation of config parameters can be found in `ConfigParams.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use Python for config defintion, but you can write your own yaml-config with an editor\n",
    "import yaml\n",
    "cfg = dict()\n",
    "# DATA DEFINITIONS ------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "cfg['DATA_TYPE'] = dict()\n",
    "# RNAseq\n",
    "cfg['DATA_TYPE']['RNA'] = dict()\n",
    "cfg['DATA_TYPE']['RNA']['SCALING'] = \"Standard\"\t# We scale features by the standard scaler\n",
    "cfg['DATA_TYPE']['RNA']['TYPE'] = \"NUMERIC\"\n",
    "cfg['DATA_TYPE']['RNA']['FILTERING'] = \"Var\"\t# We filter for feature with highest variance\n",
    "cfg['DATA_TYPE']['RNA']['FILE_RAW'] = \"combined_rnaseq_formatted.parquet\"\n",
    "\n",
    "# METH\n",
    "cfg['DATA_TYPE']['METH'] = dict()\n",
    "cfg['DATA_TYPE']['METH']['SCALING'] = \"Standard\"\n",
    "cfg['DATA_TYPE']['METH']['TYPE'] = \"NUMERIC\"\n",
    "cfg['DATA_TYPE']['METH']['FILTERING'] = \"Var\"\n",
    "cfg['DATA_TYPE']['METH']['FILE_RAW'] = \"combined_meth_formatted.parquet\"\n",
    "\n",
    "# Clinical Parameters for plotting\n",
    "cfg['DATA_TYPE']['ANNO'] = dict()\n",
    "cfg['DATA_TYPE']['ANNO']['TYPE'] = \"ANNOTATION\"\n",
    "cfg['DATA_TYPE']['ANNO']['FILE_RAW'] = \"combined_clin_formatted.parquet\"\n",
    "\n",
    "## Model and Training --------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "cfg['FIX_RANDOMNESS'] = \"all\"\n",
    "cfg['GLOBAL_SEED'] = 42\n",
    "# Model\n",
    "cfg['MODEL_TYPE'] = \"varix\" # Train a basic VAE\n",
    "cfg['TRAIN_TYPE'] = \"train\" # simple training, no tuning\n",
    "cfg['RECONSTR_LOSS'] = \"MSE\"\t# loss function for reconstruction\n",
    "cfg['VAE_LOSS'] = \"KL\"\t\t# loss function distribution distance\n",
    "cfg['BETA'] = 0.5\t\t\t# weighting of VAE loss\n",
    "\n",
    "cfg['K_FILTER'] = 1000\t\t# Input features per data modality\n",
    "cfg[\"LATENT_DIM_FIXED\"] = 6\t# Latent space dimension\n",
    "# Training\n",
    "cfg['EPOCHS'] = 500\n",
    "cfg['LR_FIXED'] = 0.0005\t# Learning rate\n",
    "cfg['BATCH_SIZE'] = 128\n",
    "cfg['DROP_P'] = 0.1\t\t\t# We have a small number of samples and should be aggressive with drop out to avoid overfitting\n",
    "# Prediction\n",
    "cfg['PREDICT_SPLIT'] = \"all\"\t# Embedding of all samples should be calculated in prediction\n",
    "# EVALUATION and VISUALIZATION ------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "cfg['DIM_RED_METH'] = \"UMAP\"\t# For 2D visualization when LATENT_DIM_FIXED>2\n",
    "cfg['CLINIC_PARAM'] = [\t\t\t# Parameters to colorize plots and perform embedding evaluation\n",
    "\t\t\t\t\"CANCER_TYPE\",\n",
    "\t\t\t\t\"CANCER_TYPE_ACRONYM\",\n",
    "\t\t\t\t\"TMB_NONSYNONYMOUS\",\n",
    "\t\t\t\t\"AGE\",\n",
    "\t\t\t\t\"SEX\",\n",
    "\t\t\t\t\"GRADE\",\n",
    "\t\t\t\t\"OS_STATUS\"\n",
    "\t\t\t]\n",
    "cfg['ML_TYPE'] = \"Auto-detect\"\t# Is CLINIC_PARAM prediction either regression or classification?\n",
    "cfg['ML_ALG'] = [\t\t\t\t# ML algorithms for embedding evaluation\n",
    "\t'Linear',\n",
    "\t'RF'\n",
    "\t]\n",
    "cfg['ML_SPLIT'] = \"use-split\"\t# Test ML performance on train, test, valid split\n",
    "cfg['ML_TASKS'] = [\t\t\t\t# Compare embeddings to other dimension reduction methods\n",
    "\t'Latent',\n",
    "\t'UMAP',\n",
    "\t'PCA',\n",
    "\t'RandomFeature'\n",
    "]\n",
    "\n",
    "\n",
    "## save config\n",
    "with open(\"../\"+\"TCGA-Example_config.yaml\", 'w') as file:\n",
    "\tyaml.dump(cfg, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Step 3) Start pipeline\n",
    "you can run the pipeline all at once with `make ml_task`, which will do the following steps:\n",
    "- `make data_only` to preprocess the input\n",
    "- `make model_only` to train the autoencoder\n",
    "- `make prediction_only` calculate latent space embedding and reconstruction of specified samples\n",
    "- `make visualize_only` create loss and latent space visualizations\n",
    "- `make ml_task_only` evaluate latent space and embedding performance\n",
    "\n",
    "In the tutorial we will make it step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Step 3A) preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /data/horse/ws/majo158f-AE-ws/autoencodix\n",
      "env: CUBLAS_WORKSPACE_CONFIG=:16:8\n",
      "/bin/bash: line 1: venv-gallia/bin/activate: No such file or directory\n",
      "Detected platform: Linux\n",
      "Setting up directories and configuration for RUN_ID: TCGA-Example\n",
      "✓ Configuration complete\n",
      "python3 src/data/make_dataset.py TCGA-Example\n",
      "2025-09-03 16:37:54,026 - src.utils.utils_basic - INFO - Unify sample ID list from data files\n",
      "2025-09-03 16:37:57,275 - src.utils.utils_basic - INFO - Save sample split.\n",
      "2025-09-03 16:37:57,305 - src.utils.utils_basic - INFO - Unified sample ID list has length 3529\n",
      "2025-09-03 16:37:57,305 - src.utils.utils_basic - INFO - Make data set ANNO\n",
      "2025-09-03 16:37:57,317 - src.utils.utils_basic - INFO - Save ANNOTATION without preprocessing\n",
      "2025-09-03 16:37:57,336 - src.utils.utils_basic - INFO - Make data set METH\n",
      "2025-09-03 16:37:58,082 - src.utils.utils_basic - INFO - Select for samples and drop features with NA\n",
      "2025-09-03 16:37:58,259 - src.utils.utils_basic - INFO - Filter data: METH\n",
      "2025-09-03 16:37:58,260 - src.utils.utils_basic - INFO - Filter by variance and exclude with no variance\n",
      "2025-09-03 16:37:58,820 - src.utils.utils_basic - INFO - Normalize data: METH\n",
      "2025-09-03 16:37:58,821 - src.utils.utils_basic - INFO - Normalize features by Standard\n",
      "2025-09-03 16:37:58,846 - src.utils.utils_basic - INFO - Save data: METH\n",
      "2025-09-03 16:37:58,846 - src.utils.utils_basic - INFO - Shape of METH data after cleaning: (3529, 1000)\n",
      "2025-09-03 16:37:59,106 - src.utils.utils_basic - INFO - Make data set RNA\n",
      "2025-09-03 16:38:00,599 - src.utils.utils_basic - INFO - Select for samples and drop features with NA\n",
      "2025-09-03 16:38:01,088 - src.utils.utils_basic - INFO - Filter data: RNA\n",
      "2025-09-03 16:38:01,088 - src.utils.utils_basic - INFO - Filter by variance and exclude with no variance\n",
      "2025-09-03 16:38:01,811 - src.utils.utils_basic - INFO - Normalize data: RNA\n",
      "2025-09-03 16:38:01,811 - src.utils.utils_basic - INFO - Normalize features by Standard\n",
      "2025-09-03 16:38:01,834 - src.utils.utils_basic - INFO - Save data: RNA\n",
      "2025-09-03 16:38:01,834 - src.utils.utils_basic - INFO - Shape of RNA data after cleaning: (3529, 1000)\n",
      "echo \"done data\"\n",
      "done data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Make sure to start from project root folder\n",
    "project_path = os.path.expandvars(\"$HOME/horse/$USER-AE-ws/autoencodix\")\n",
    "os.chdir(project_path)\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Setting CUBLAS for reproducibility if you train on GPU\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "# Activate venv\n",
    "!source venv-gallia/bin/activate\n",
    "# Start preprocessing\n",
    "!make data RUN_ID=TCGA-Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Step 3B) train the variational autoencoder (Varix)\n",
    "this may take some time depending on your machine and GPU (around 2min for a RTX 2070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:16:8\n",
      "Detected platform: Linux\n",
      "Setting up directories and configuration for RUN_ID: TCGA-Example\n",
      "✓ Configuration complete\n",
      "python3 src/models/train.py TCGA-Example\n",
      "2025-09-03 16:40:39,634 - src.utils.utils_basic - INFO - DATA SIZE:torch.Size([2470, 2000])\n",
      "2025-09-03 16:40:40,517 - src.utils.utils_basic - INFO - total epochs:500\n",
      "2025-09-03 16:40:59,377 - src.utils.utils_basic - INFO - Train Set: Epoch: 0, Loss: 2061.01, r_Loss: 2059.70, vae_loss: 1.31, r2: -0.07\n",
      "2025-09-03 16:40:59,382 - src.utils.utils_basic - INFO -  Valid Set: Epoch: 0 Loss: 11657.23, r2: -0.00\n",
      "2025-09-03 16:41:03,296 - src.utils.utils_basic - INFO - Train Set: Epoch: 50, Loss: 1101.57, r_Loss: 1101.22, vae_loss: 0.35, r2: 0.42\n",
      "2025-09-03 16:41:03,299 - src.utils.utils_basic - INFO -  Valid Set: Epoch: 50 Loss: 1508.53, r2: 0.43\n",
      "2025-09-03 16:41:07,212 - src.utils.utils_basic - INFO - Train Set: Epoch: 100, Loss: 1063.29, r_Loss: 1058.83, vae_loss: 4.47, r2: 0.44\n",
      "2025-09-03 16:41:07,215 - src.utils.utils_basic - INFO -  Valid Set: Epoch: 100 Loss: 2049.51, r2: 0.44\n",
      "2025-09-03 16:41:11,161 - src.utils.utils_basic - INFO - Train Set: Epoch: 150, Loss: 1062.62, r_Loss: 1044.79, vae_loss: 17.82, r2: 0.45\n",
      "2025-09-03 16:41:11,164 - src.utils.utils_basic - INFO -  Valid Set: Epoch: 150 Loss: 1440.52, r2: 0.44\n",
      "2025-09-03 16:41:15,021 - src.utils.utils_basic - INFO - Train Set: Epoch: 200, Loss: 1087.35, r_Loss: 1054.67, vae_loss: 32.67, r2: 0.44\n",
      "2025-09-03 16:41:15,026 - src.utils.utils_basic - INFO -  Valid Set: Epoch: 200 Loss: 937.64, r2: 0.43\n",
      "2025-09-03 16:41:18,895 - src.utils.utils_basic - INFO - Train Set: Epoch: 250, Loss: 1134.16, r_Loss: 1079.96, vae_loss: 54.21, r2: 0.43\n",
      "2025-09-03 16:41:18,898 - src.utils.utils_basic - INFO -  Valid Set: Epoch: 250 Loss: 836.35, r2: 0.42\n",
      "2025-09-03 16:41:22,562 - src.utils.utils_basic - INFO - Train Set: Epoch: 300, Loss: 1155.71, r_Loss: 1090.89, vae_loss: 64.82, r2: 0.42\n",
      "2025-09-03 16:41:22,566 - src.utils.utils_basic - INFO -  Valid Set: Epoch: 300 Loss: 823.29, r2: 0.41\n",
      "2025-09-03 16:41:26,474 - src.utils.utils_basic - INFO - Train Set: Epoch: 350, Loss: 1156.42, r_Loss: 1090.67, vae_loss: 65.75, r2: 0.42\n",
      "2025-09-03 16:41:26,477 - src.utils.utils_basic - INFO -  Valid Set: Epoch: 350 Loss: 816.16, r2: 0.41\n",
      "2025-09-03 16:41:30,354 - src.utils.utils_basic - INFO - Train Set: Epoch: 400, Loss: 1150.85, r_Loss: 1084.34, vae_loss: 66.50, r2: 0.42\n",
      "2025-09-03 16:41:30,357 - src.utils.utils_basic - INFO -  Valid Set: Epoch: 400 Loss: 812.94, r2: 0.41\n",
      "2025-09-03 16:41:34,270 - src.utils.utils_basic - INFO - Train Set: Epoch: 450, Loss: 1144.17, r_Loss: 1078.38, vae_loss: 65.79, r2: 0.42\n",
      "2025-09-03 16:41:34,274 - src.utils.utils_basic - INFO -  Valid Set: Epoch: 450 Loss: 806.12, r2: 0.42\n",
      "2025-09-03 16:41:38,301 - src.utils.utils_basic - INFO - Model saved as models/TCGA-Example/METH_RNA_varixTCGA-Example.pt\n",
      "2025-09-03 16:41:38,302 - src.utils.utils_basic - INFO - Model structure:\n",
      "2025-09-03 16:41:38,303 - src.utils.utils_basic - INFO - Varix(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=2000, out_features=500, bias=True)\n",
      "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=500, out_features=6, bias=True)\n",
      "  (logvar): Linear(in_features=500, out_features=6, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=500, bias=True)\n",
      "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=500, out_features=2000, bias=True)\n",
      "  )\n",
      ")\n",
      "echo \"Done training only\"\n",
      "Done training only\n"
     ]
    }
   ],
   "source": [
    "# Setting CUBLAS for reproducibility\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "\n",
    "!make model_only RUN_ID=TCGA-Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Step 3C) calculate latent space embeddings of all samples\n",
    "for visualization and evaluation latant space for all samples are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected platform: Linux\n",
      "Setting up directories and configuration for RUN_ID: TCGA-Example\n",
      "✓ Configuration complete\n",
      "python3 src/models/predict.py TCGA-Example\n",
      "2025-09-03 16:41:56,141 - src.utils.utils_basic - INFO - ['METH', 'RNA']\n",
      "2025-09-03 16:41:59,042 - src.utils.utils_basic - INFO - DATA SIZE:torch.Size([3529, 2000])\n",
      "2025-09-03 16:41:59,079 - src.utils.utils_basic - INFO - Model structure:\n",
      "2025-09-03 16:41:59,080 - src.utils.utils_basic - INFO - Varix(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=2000, out_features=500, bias=True)\n",
      "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=500, out_features=6, bias=True)\n",
      "  (logvar): Linear(in_features=500, out_features=6, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=500, bias=True)\n",
      "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=500, out_features=2000, bias=True)\n",
      "  )\n",
      ")\n",
      "echo \"Done predicting only\"\n",
      "Done predicting only\n"
     ]
    }
   ],
   "source": [
    "!make prediction_only RUN_ID=TCGA-Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Step 3D) Visualize results\n",
    "plots for loss and latent space are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected platform: Linux\n",
      "Setting up directories and configuration for RUN_ID: TCGA-Example\n",
      "✓ Configuration complete\n",
      "python3 src/visualization/visualize.py TCGA-Example\n",
      "2025-09-03 16:42:08,815 - src.utils.utils_basic - INFO - Plotting loss per type over epochs.\n",
      "2025-09-03 16:42:08,817 - src.utils.utils_basic - INFO - Plotting loss for reports/TCGA-Example/losses_METH_RNA_varix.parquet\n",
      "2025-09-03 16:42:12,599 - src.utils.utils_basic - INFO - Performing UMAP for 2D latent space visualization.\n",
      "2025-09-03 16:42:26,298 - src.utils.utils_basic - INFO - Plot UMAP with SPLIT as labels.\n",
      "2025-09-03 16:42:27,144 - src.utils.utils_basic - INFO - Plot UMAP with CANCER_TYPE as labels.\n",
      "2025-09-03 16:42:27,461 - src.utils.utils_basic - INFO - Plot UMAP with CANCER_TYPE_ACRONYM as labels.\n",
      "2025-09-03 16:42:27,959 - src.utils.utils_basic - INFO - Plot UMAP with TMB_NONSYNONYMOUS as labels.\n",
      "2025-09-03 16:42:27,960 - src.utils.utils_basic - INFO - The provided label column is numeric and converted to categories.\n",
      "2025-09-03 16:42:28,263 - src.utils.utils_basic - INFO - Plot UMAP with AGE as labels.\n",
      "2025-09-03 16:42:28,264 - src.utils.utils_basic - INFO - The provided label column is numeric and converted to categories.\n",
      "2025-09-03 16:42:28,553 - src.utils.utils_basic - INFO - Plot UMAP with SEX as labels.\n",
      "2025-09-03 16:42:28,838 - src.utils.utils_basic - INFO - Plot UMAP with GRADE as labels.\n",
      "2025-09-03 16:42:29,148 - src.utils.utils_basic - INFO - Plot UMAP with OS_STATUS as labels.\n",
      "2025-09-03 16:42:29,431 - src.utils.utils_basic - INFO - Plot latent dim distributions\n",
      "2025-09-03 16:42:29,431 - src.utils.utils_basic - INFO - Plot latent for CANCER_TYPE\n",
      "2025-09-03 16:42:30,795 - src.utils.utils_basic - INFO - Plot latent for CANCER_TYPE_ACRONYM\n",
      "2025-09-03 16:42:31,905 - src.utils.utils_basic - INFO - Plot latent for TMB_NONSYNONYMOUS\n",
      "2025-09-03 16:42:32,827 - src.utils.utils_basic - INFO - Plot latent for AGE\n",
      "2025-09-03 16:42:34,019 - src.utils.utils_basic - INFO - Plot latent for SEX\n",
      "2025-09-03 16:42:34,858 - src.utils.utils_basic - INFO - Plot latent for GRADE\n",
      "2025-09-03 16:42:35,837 - src.utils.utils_basic - INFO - Plot latent for OS_STATUS\n"
     ]
    }
   ],
   "source": [
    "!make visualize_only RUN_ID=TCGA-Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "For evaluation of model training and autoencoder performance we generate multiple visualizations:  \n",
    "- Loss curves\n",
    "- Ridgeline (density) plots of each latent dimension grouped by annotation parameters\n",
    "- 2D latent space representations colored by annotation parameters\n",
    "\n",
    "Optionally to those standard plots, you can activate via config parameters additional plots:\n",
    "- Heatmap of model weights before and after training (config: `PLOT_WEIGHTS`)\n",
    "- 2D representation of direct input embedding by UMAP, PCA or t-SNE for comparison (config: `PLOT_INPUT2D`)\n",
    "- Clustering and visualization of 2D latent space representation (config: `PLOT_CLUSTLATENT`)\n",
    "- Plot 2D latent representation at each checkpoint during training (config: `CHECKPT_PLOT`)\n",
    "\n",
    "In the following we show the standard visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "loss_absolute = Image(filename='reports/TCGA-Example/figures/loss_plot_absolute.png', width=800)\n",
    "loss_relative = Image(filename='reports/TCGA-Example/figures/loss_plot_relative.png', width=800)\n",
    "\n",
    "print(\"Loss curves on log-scale\")\n",
    "display(loss_absolute)\n",
    "print(\"Relative contribution of loss terms to total loss\")\n",
    "display(loss_relative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Ridgeline (density) plots\n",
    "\n",
    "Note the difference of latent dimensions and how they seperate cancer types or female/male patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_subtype = Image(filename='reports/TCGA-Example/figures/latent_dist_CANCER_TYPE.png', width=800)\n",
    "\n",
    "ridge_grade = Image(filename='reports/TCGA-Example/figures/latent_dist_SEX.png', width=800)\n",
    "\n",
    "print(\"Latent intensity of each dimension grouped by cancer subtypes\")\n",
    "display(ridge_subtype)\n",
    "print(\"Latent intensity of each dimension grouped by female/male patients\")\n",
    "display(ridge_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 2D Latent space representation (via UMAP)  \n",
    "UMAP is used to reduce from six latent dimensions to two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent2D_subtype = Image(filename='reports/TCGA-Example/figures/latent2D_CANCER_TYPE.png', width=800)\n",
    "latent2D_grade = Image(filename='reports/TCGA-Example/figures/latent2D_SEX.png', width=800)\n",
    "\n",
    "print(\"2D latent space colored by cancer subtypes\")\n",
    "display(latent2D_subtype)\n",
    "print(\"2D latent space colored by female/male patients\")\n",
    "display(latent2D_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Step 3E) Evaluate embedding performance by machine learnig tasks\n",
    "For the given clinical parameters in the config, we can test our embeddings in comparison to other methods like PCA, UMAP or random feature picking as baseline  \n",
    "This may take some time, especially since random feature picking is repeated five times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!make ml_task_only RUN_ID=TCGA-Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in a text file `ml_task_performance.txt` and visualized in plots for each clinical parameter\n",
    "\n",
    "For each parameter latent space of trained varix is compared to embedding by PCA, UMAP or random feature picks on each data split created in `make data`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_subtype = Image(filename='reports/TCGA-Example/figures/ml_task_performance_CANCER_TYPE_roc_auc_ovo.png', width=800)\n",
    "ml_fm = Image(filename='reports/TCGA-Example/figures/ml_task_performance_SEX_roc_auc_ovo.png', width=800)\n",
    "\n",
    "\n",
    "print(\"Subtype classification performance\")\n",
    "display(ml_subtype)\n",
    "print(\"Female/male classification performance\")\n",
    "display(ml_fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First row shows performance on linear models (linear regression or logistic regression) and second row on RandomForest as machine learning algorithm.  \n",
    "\n",
    "We see that cancer type classification is fairly easy and all methods VAE, UMAP and PCA do very well. Female/male classification is slightly harder on the omics data and VAEs are again as performant as other methods. \n",
    "\n",
    "Check out the Ontix tutorial to see how biologically-informed VAEs can significantly improve performance for that case. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencodix-ga-kernel",
   "language": "python",
   "name": "autoencodix-ga-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
